# Context API

## Table of Contents

[Task Script Entry-Point Function Example](#task-script-entry-point-function-example)
[Context API Properties and Functions](#context-api-properties-and-functions)
     [Introduction](#Introduction)
     [Properties](#Properties)
     [context.read_file](#contextread_file)
     [context.write_file](#contextwrite_file)
     [context.write_ids](#contextwrite_ids)
     [context.get_file_name](#contextget_file_name)
     [context.get_ids](#contextget_ids)
     [context.validate_ids](#contextvalidate_ids)
     [context.get_logger](#contextget_logger)
     [context.get_secret_config_value](#contextget_secret_config_value)
     [context.get_presigned_url](#contextget_presigned_url)
     [context.run_command](#contextrun_command)
     [context.update_metadata_tags](#contextupdate_metadata_tags)
     [context.resolve_secret](#contextresolve_secret)
     [context.add_labels](#contextadd_labels)
     [context.get_labels](#contextget_labels)
     [context.delete_labels](#contextdelete_labels)
     [context.add_attributes](#contextadd_attributes)


## Introduction
To interact with Tetra Data Platform task scripts you will need to use the context object. The context object provides functions for you to read files, write files, get pipeline configuration information, and more. 
This topic contains:

* An example of a task script entry-point function
* A description of the context API properties and functions
* A description of Local Context API functions for unit tests.
* Task Script Entry-Point Function Example

If your task script entry-point function process_file is in file main.py, you should define process_file like this:

```py
# main.py

def process_file(input: dict, context: object):
    print("Starting task")
    
    input_data = context.read_file(input["inputFile"])
    # your business logic ...
    
    print("Task completed")
```

You can define input in the protocol' script.js. But, you don't need to define context. It will be passed in as the second argument automatically.

## Context API Properties and Functions
Context API provides properties and functions that allow you to read and write files and more.

### Properties Table
|Parameter|Type|Description|
|---------|----------|---------|
|context.org_slug|	string|	Your organization's slug.|
|context.pipeline_id|	uuid|	Pipeline unique ID.|
|context.workflow_id|	uuid|	Workflow unique ID.|
|context.master_script_namespace|	string|	Namespace for the protocol script.|
|context.master_script_slug|	string|	Slug for the protocol script (script.js).|
|context.master_script_version|	string|	Version of the protocol script (e.g. v1.0.0).|
|context.protocol_slug|	string|	Protocol slug.|
|context.protocol_version|	string|	Protocol version.|
|context.pipeline_config|	dict|	A dictionary from strings to strings, containing the configuration parameters set when configuring the pipeline that invoked this protocol.|
|context.input_file|	dict|	File pointer that triggered the current workflow.|
|context.created_at	|string|	Workflow's creation time.ISO timestamp string. Generated by moment().toISOString(). Indicates when the file is created.|
|context.task_id|	uuid|	ID for the current task (step of a pipeline).|
|context.task_created_at|	string|	Task's creation time. ISO timestamp string. Generated by moment().toISOString(). Indicates whent the task is created.|
|context.platform_url|	string|	The URL of the platform (i.e.: https://platform.tetrascience.com).|
|context.tmp_dir|	string|	Folder for tmp dir. Usually it‚Äôs `/tmp`. If your task script wants to write a file and then call a shell command (like LibreOffice) to process the file, you should put it in context.tmp_dir . Other folders are not guaranteed to be writeable.|
|context.read_file|string|Takes an input file reference and returns a dictionary with keys.|

### Parameter(s)
|Parameter|Type|Description|
|---------|----------|---------|
|file|	dict|	File Pointer|
|form|	string|	Optional. Indicates the format for the result. By default, the form is set to body, but it can also be file_obj and download.|
### Examples
*body*

```py

f = context.read_file(input_file_pointer, form='body')
json.loads(f['body'])
```

*file_obj*

```py

f = context.read_file(input_file_pointer, form='file_obj')
with zipfile.ZipFile(f['file_obj']) as zf:
  with zf.open("somefile.csv", "r") as csvf:
    pd.read_csv(csvf)
```

*download*

```py

f = context.read_file(input_file_pointer, form='download')
con = sqlite3.connect(f['download'])
df = pd.read_sql_query('SELECT * FROM foo', con)
```

### Return(s)
Returns an dictionary, as listed below:

* metadata: dict. S3 file metadata - note that custom metadata and tags shown in the platform UI are encoded within the S3 metadata, but there are additional metadata fields as described in Integrations and Sources. The metadata dictionary in python keys specified by this mapping: [https://github.com/tetrascience/ts-sdk-python/blob/main/ts_sdk/task/__util_metadata.py](https://github.com/tetrascience/ts-sdk-python/blob/main/ts_sdk/task/__util_metadata.py).
* body, file_obj or download - exactly one of these keys is present, depending on the value of the form parameter.
   * result['body'] holds the contents of the file as a byte array. This approach cannot handle large files that don't fit in memory.
   * result['file_obj'] is a file-like object that can be used to access the body in a streaming manner. This object can be passed to Python libraries such as Pandas.
   * result['download'] is the file name of a local file that has been downloaded from the specified data lake file. This is useful when the data needs to be processed by native code (e.g. SQLite) or an external utility program.
* custom_metadata: custom metadata of the document
* custom_tags: list of custom tags of the document

## context.write_file
Writes an output file to the data lake.

### Parameters
The following are the parameters for this endpoint.

|Parameter|Type|Description|
|---------|----------|---------|
|content|	string, bytes object, or byte stream|	The content to be written out.|
|file_name|	string|	The name of the file to be written. (The full S3 path is determined by the platform.)|
|file_category|	string|	File category, can be "IDS", "TMP" or "PROCESSED"|
|ids|	string|	Optional. If file_category is "IDS", then this parameter specifies the specific Intermediate Data Schema. The format is: namespace/slug:v1.2.3.  Default is `None`.|
|custom_metadata|	dict of string to string|	(Optional. Default is `{}`.) Metadata to be appended to a file. Keys can only include letters, numbers, spaces, and the symbols `+, -, /, or .`. Values can only include letters, numbers, spaces, and the symbols `+, -, , /, . or ,`.|
|custom_tags|	list of strings|Optional. (Default: `[]`)  	Tags to be appended to a file. Can only include letters, numbers, spaces, and the symbols `+, -, ., /, or _)`. Limited to 128 characters.|
|source_type|	string|	Optional. (Default: `None`) Can be used to overwrite the source type S3 metadata value of the resulting document. Validation is performed if a value is passed against the regex `/^[-a-z0-9]+$/`, if no value is passed, we fall back to the default logic (take source type value from the RAW file, or if not found on RAW file, set the value unknown).|
|labels| list of dict| Optional. (Default: `[]`)  List of dictionaries where each dictionary is in form `{ 'name': <LABEL_NAME>, 'value': <LABEL_VALUE> }`.  Names can only include letters, numbers, spaces, and the symbols` +, -, or _`. Values can only include letters, numbers, spaces, and the symbols `+, -, _, /, . or ,`.

### Examples
*content: string, bytes object, byte stream or string*

*string*

```py

context.write_file(
    content='this is a string',
    file_name=file_name,
    file_category=file_category
)
```

*bytes object*

```py

context.write_file(
    content=bytes('byte content', 'utf-8'),
    file_name=file_name,
    file_category=file_category
)
# or
context.write_file(
    content=b'byte content',
    file_name=file_name,
    file_category=file_category
)
```

*byte stream / file object*

üìò NOTE:  If a gzipped file is larger than 100MB, the file will be uploaded as multi-part upload.

```py

with zf.open("somefile.csv", "rb") as f:
    context.write_file(
        content=f,
        file_name=file_name,
        file_category=file_category
    )
```

### Returns
Returns an object that indicates the file location.

Here is an example:

```py


{ 
  "type": "s3file", 
  "bucket": "datalake", 
  "fileKey": "path/to/file", 
  "version": "versionId"
}
```
üìò NOTE: `version` will not be available when you are running pipelines locally.

## context.write_ids
Writes an output IDS file to the data lake.

üöß WARNING:  Notice you can only define file_suffix, you cannot define the complete file name of the output IDS file. The file name is codified and follows a certain pattern, which includes ids_version. This means if you upgraded your raw-to-ids protocol and the new protocol is using a new IDS version, the new IDS JSON produced will be a separate file not a new version of the old file.

### Parameters
This function contains the following parameters.

|Parameter|Type|Description|
|---------|----------|---------|
|content_obj|dict|	Dictionary that can be JSON-serialized.|
|file_suffix|	string|	String that will be appended to the IDS file name generated from the provided information (IDS type and version). Useful for keeping names unique when a task script generates multiple IDS files of the same type. Document will have a systematic file name generated: `${idstype}${idsversion}${file_suffix}`.|
|ids|	string|	Optional. (Default: `None`). If the file_category is "IDS", then this parameter specifies the specific Intermediate Data Schema. The format is: `namespace/slug:v1.2.3`.|
|custom_metadata|	dict of string to string|	Optional. (Default:`{}`) Metadata to be appended to a file. Keys can only include letters, numbers, spaces, and the symbols `+, -, /, or .`. Values can only include letters, numbers, spaces, and the symbols `+, -, , /, . or ,`.
|custom_tags|	list of strings| Optional (Default:`[]`). Tags to be appended to a file. Can only include letters, numbers, spaces, and the symbols `+, -, ., /, or _)`. Limited to 128 characters.|
|source_type|	string|	Optional. (Default: `None`). Used to overwrite the source type S3 metadata value of the resulting document. Validation is performed if a value is passed against the regex `/^[-a-z0-9]+$/`, if no value is passed, we fall back to the default logic (take source type value from the RAW file, or if not found on RAW file, set the value unknown).|
|file_category|	string|	Optional. (Default:`IDS`). Overwrites file category. Allowed values are IDS and TMP. If value is not provided or if other values is provided, it will be defaulted to IDS.|
|labels| List of Dict| Optional. (Default:`[]`) List of dictionaries where each dictionary is in form ` { 'name': <LABEL_NAME>, 'value': <LABEL_VALUE> }`. names can only include letters, numbers, spaces, and the symbols `+, -, or _`. values can only include letters, numbers, spaces, and the symbols `+, -, _, /, . or ,`.

### Returns
Returns an object that describes the file location. E.g. `{ type: 's3file', bucket: 'datalake', fileKey: 'path/to/file', version: 'versionId'}`. Note that version is not available when running pipelines locally.

## context.get_file_name
Use this function to retrieve filename of a file that is not downloaded locally.

### Parameters
The following table shows the parameters for this function.

|Parameter|Type|Description|
|---------|----------|---------|
|file|	dict|	File pointer|

### Returns
A string; the filename of the file. Note that you do not need to download the file locally to access the file name.

## context.get_ids
Use this function to get the Intermediate Data Schema that matches the namespace, slug, and version.

### Parameters
The parameters for this function appear in the following table.

|Parameter|Type|Description|
|---------|----------|---------|
|namespace|	string|	A namespace defines a realm; only those with the appropriate permissions can use the artifacts in that realm. There are three main categories of namespace: common, client, and private.|
|slug|	string|	A slug is a unique identifier. The term is also used to denote a reference to a unique identifier (pointer).|
|version|	string|	Version of the IDS.|

### Returns
Returns an IDS schema object.

## context.validate_ids
Checks the validity of IDS content provided in data. Throws an error if not valid.

### Parameters
|Parameter|Type|Description|
|---------|----------|---------|
|data|	dict|	The JSON content of the IDS file.|
|namespace|	string|	A namespace defines a realm; only those with the appropriate permissions can use the artifacts in that realm. There are three main categories of namespace: common, client, and private|
|slug|	string|	A slug is a unique identifier. The term is also used to denote a reference to a unique identifier (pointer).|
|version|	string|	Version of the IDS.|

### Returns
Returns a boolean value indicating whether the IDS is valid (true). It throws an error if the IDS is not valid.

## context.get_logger
Returns the logger object, which currently has one method: log(data).

### Parameters
There are no parameters for this function.

### Example(s)
*log an object*

```py

logger = context.get_logger()
logger.log({
    "message": "Starting the main parser",
    "level": "info"
})
# log output
# {"timestamp": "2020-04-22T20:45:02.540947", "message": "Starting the main parser", "level": "info"}
log a string, the default level is ‚Äúinfo‚Äù


logger = context.get_logger()
logger.log("Writing IDS into the datalake")
# log output
# {"timestamp": "2020-04-22T20:45:02.971038", "message": "Writing IDS into the datalake", "level": "info"}
```
### Returns
This returns the structured logger object, which currently has one method: `log(input)`.

## context.get_secret_config_value
üìò NOTE: We recommend passing pipeline configs to your task script function in master-script script.js. Then in your task script function, use `context.resolve_secret` to resolve the secret value. This makes your task script function less dependent on TS `context` function (more like a pure function). Please see the documentation for that function.

Retrieves the secret's value.

### Parameters
|Parameter|Type|Description|
|---------|----------|---------|
|secret_name|	string|	Secret name.|
|silent_on_error|	boolean|	Optional. Default: True. If set to True and the secret is missing, this function will return an empty string (otherwise an error will be thrown).|

### Returns
The value of the secret. Note that the secret-name parameter must be passed and must be a string. If you do not enter the secret-name parameter, and if it is not a string, an error will be thrown. If you enter a string, but it is not the name of an existing secret, an empty string will be returned.

## context.get_presigned_url
Returns a time-limited HTTPS URL that can be used to access the file. If URL generation fails for any reason (except invalid value for `ttl_sec` parameter). None will be returned.

### Parameters
Parameters for this function appear in the following table.

|Parameter|Type|Description|
|---------|----------|---------|
|file|	dict|	File pointer|
|ttl_sec|	number| Optional. (Default: `300`)	How long the URL will be valid before it expires. It is recommended that task scripts adjust this to be in line with the command TTL. Optional, defaults to 300. Must be between 0 and 900, otherwise an error is thrown.|

### Returns
String that contains a time-limited HTTPS URL that can be used to access the file.

## context.run_command
Invokes remote command/action on target (agent or connector) and returns its response.

üìò NOTE: This function is still in the early stage and requires Tetra DataHub and Agent support. It's not an out-of-shelf function that you can use on any instrument. Please contact the TetraScience team for more information.

### Parameters
|Parameter|Type|Description|
|---------|----------|---------|
|org_slug|	string|	organization slug|
|target_id|	string|	Unique Identifier; identifies the connector or agent that will receive the command. It is recommended that task scripts receive this as a pipeline config parameter|
|action|	string, enum|	See supported actions below.|
|payload|	dict|	JSON object specific to the selected action|
|metadata|	dict| Optional. (Default: `{}`)	A dict that describes command. Method will automatically add values for workflowId, pipelineId and taskId to metadata that will be sent to command service for execution.|
|ttl_sec|	number| Optional.  (Default: `300`) It is recommended that task scripts receive this as a pipeline config parameter. Range: 300 seconds (5min) to 900 seconds (15min)|

### Actions
`TetraScience.Connector.gdc.HttpRequest`
Send REST call to any network address as long as that address is reachable to GDC.
* Requires TDP v3.0.0 and above
* Requires DataHub and Generic Data Connector (GDC)
* No agent needed

### Returns
Returns a JSON response object returned by target (agent or connector) that ran the command.

It throws an error when the following are missing: `org_slug`, `target_id`, `action`, `payload`, and `ttl_sec`. Also throws an error when the `ttl_sec` is less than 300 seconds or higher than 900 seconds.

Error messages include the following:

* command not created successfully
* TTL has expired
* command didn‚Äôt execute successfully

## context.update_metadata_tags

Updates file's metadata and tags.

### Parameters
|Parameter|Type|Description|
|---------|----------|---------|
|file|	dict|	File pointer|
|custom_meta|	dict|	Optional. (Default: `{}`)  Updates the metadata. Passing "None" for this parameter removes the metadata entry.|
|custom_tags|	string|	Optional. (Default: `[]`) List of new tags.

### Returns
Returns an object that describes the file location.

```py

{
  type: 's3file',
  bucket: 'bucket',
  fileKey: 'fileKey',
  fileId: 'fileId',
  // no version locally (from fakes3)
  version: 'versionId'
}
```

## context.resolve_secret
Returns the secret value. This function is used to convert the SSM reference to the actual secret value.

### Parameters
|Parameter|Type|Description|
|---------|----------|---------|
|secret_ref|	dict|	It contains SSM parameter store path to the secret value.|

### Example
If you defined a secret in protocol.json as:
```json
JSON

{
...
  config: [{
    "slug": "password",
    "name": "Password",
    "description": "This is a password",
    "type": "secret",
    "required": true
  }],
...
}
```
Then in the script.js, you will get password reference from the pipeline config:

```js
JavaScript

...
const pipelineConfig = workflow.getContext('pipelineConfig');
const { password } = pipelineConfig;
await workflow.runTask('task-1', { password });
...
```

Then in your task script, you can get the password reference from input["password"]. The reference is a dictionary that contains SSM parameter store path to the secret value, like this:

```py

{
  "secret": true,
  "ssm": "/development/tetrascience-demo/org-secrets/password",
  "version": 1
}
```
```py

def main(input, context):
  password = context.resolve_secret(input.get('password'))
  ...
  ```

### Returns
Returns the secret value.

The secret parameter must be a ‚Äòdict‚Äô type (an associative array - key:value). If it is not, or if the value is not a secret, the parameter that you entered will be returned.

Note: An example of the format of a valid value is `{"ssm":"/file/path", "secret":true}`.

## context.add_labels
Allows you to add labels to a file.

### Parameter
Parameters appear in the table below.

|Parameter|Type|Description|
|---------|----------|---------|
|file|	dict|	Contains either (1) fileId or (2) bucket, fileKey and version (optional) of a file you want to add labels to.|
|labels|list of dict|	List of dictionaries where each dictionary is in form `{ 'name': <LABEL_NAME>, 'value': <LABEL_VALUE> }`|

### Returns
Returns an array of added labels, e.g. `[{"name": "Label A", "value": "Label B", "id": 22954, "createdAt": "2021-03-07T20:27:06.466Z"}]`

## context.get_labels
Use this function to get all of the labels associated with a file.

|Parameter|Type|Description|
|---------|----------|---------|
|file|	dict|	Contains either (1) fileId or (2) bucket, fileKey and version (optional) of a file that you want view label information for.|

### Returns
Returns an array of labels for given file, e.g. `[{"name": "Label A", "value": "Label B", "id": 22954, "createdAt": "2021-03-07T20:27:06.466Z"}]`

## context.delete_labels
Use this function to delete one or more labels from a file. Labels are discussed in detail in the Basic Concepts: Metadata, Tags, and Labels topic.

|Parameter|Type|Description|
|---------|----------|---------|
|file|	dict|	Can contain either (1) fileId or (2) bucket, fileKey and version (optional) of a file that we want to delete labels from.|
|label_ids|	strings|	IDs of the labels you want to delete.|

### Returns
An empty object if the label was successfully deleted from the file.

## context.add_attributes
This function has been added with TDP v3.0.0 and ts-sdk v1.2.20. This function allows you to add metadata, tags, or labels to an object.

|Parameter|Type|Description|
|---------|----------|---------|
|file|	dict|	A dict that describes the file location. E.g. `{ type: 's3file', bucket: 'datalake', fileKey: 'path/to/file'}`|
|custom_meta|	dict|	(Optional, default: {}). List of metadata values and keys. Passing in a value of "None" will remove all metadata from the file.|
|custom_tags|	list|	(Optional, default: [])  List of tags.|
|labels|	list|	(Optional, default [])  List of dictionaries where each dictionary is in form `{ 'name': <LABEL_NAME>, 'value': <LABEL_VALUE> }`|

### Returns
Returns a dict that describes the file location. A new file version will be generated if metadata/tags were provided.
